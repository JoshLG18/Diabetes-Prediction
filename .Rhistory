#Table of all performance metrics
# Create a dataframe for the performance metrics
performance_df <- data.frame(
Model = c("Random Forest", "SVM", "Logistic Regression"),
Accuracy = c(accuracyRF, accuracySVM, accuracyLR),
Precision = c(precisionRF, precisionSVM, precisionLR),
Recall = c(recallRF, recallSVM, recallLR),
F1_Score = c(f1_scoreRF, f1_scoreSVM, f1_scoreLR),
AUC = c(auc_value_RF, auc_value_SVM, auc_value_LR)
)
# Display table using kable for better formatting
kable(performance_df, caption = "Performance Metrics for Each Model")
mush <- mush %>%
mutate(edible = ifelse(class == 'e', 1, 0))
table(mush$class, mush$edible)
mush_d <- fastDummies::dummy_cols(mush) %>%
select_if(is.numeric) %>%
select(-edible, -class_p, -class_e)
mush_pca <- FactoMineR::PCA(mush_d, ncp = 20)
library(factoextra)
fviz_eig(mush_pca, ncp = 20)
fviz_pca_ind(mush_pca, geom = 'point', alpha = 0.2)
mush_pca_d <- mush_pca$ind$coord %>%
as.data.frame()
mush_pca_d$edible <- as.factor(mush$edible)
ggplot(mush_pca_d) +
geom_point(aes(x = Dim.1, y = Dim.2, color = edible), alpha = 0.3) +
scale_color_manual(values = c('firebrick','dodgerblue4')) +
theme_minimal()
table(mush$edible) / length(mush$edible)
x <- table(mush$edible) / length(mush$edible)
-( x[1] * log2(x[1]) + x[2] * log2(x[2]))
entropy <- function(x) {
x <- table(x) / length(x)
-1 * sum(sapply(x, function(q) q * log2(q)))
}
entropy(mush$edible)
entropy(mush$class)
entropy(mush$gill_color)
entropy_plot <- function(v = 'gill_color')  {
d <- mush
d$f <- d[[v]]
d <- d %>%
group_by(f) %>%
summarize(entropy = entropy(class), n = n()) %>%
mutate(p = n / sum(n)) %>%
arrange(p, entropy) %>%
mutate(f = fct_reorder(f, entropy))
print(d)
d <- d %>%
mutate(f_num = as.numeric(f)) %>%
arrange(f_num) %>%
mutate(lag_p = lag(p)) %>%
mutate(lag_p = ifelse(is.na(lag_p), 0, lag_p)) %>%
mutate(x_min = cumsum(lag_p)) %>%
mutate(x_max = lead(x_min)) %>%
mutate(x_max = ifelse(is.na(x_max), 1, x_max))
ggplot(d) +
geom_rect(ymin = 0, aes(xmin = x_min, xmax = x_max, ymax = entropy),
color = 'grey50', fill = 'lightblue') +
geom_text(y = 0, vjust = -1,
aes(x = ((x_max - x_min) / 2)  + x_min, label = f)) +
scale_x_continuous(name = v, breaks = c(0, 1), limits = c(0, 1)) +
scale_y_continuous(breaks = c(0, 1), limits = c(0, 1)) +
theme_minimal() +
theme(axis.ticks = element_blank(), panel.grid.minor = element_blank())
}
entropy_plot('gill_color')
library(rpart)
library(rpart.plot)
mush_tree <- rpart(class ~ ., data = mush)
mush_tree
rpart.plot(mush_tree)
x <- tibble(var_name = names(mush_tree$variable.importance),
importance = mush_tree$variable.importance) %>%
mutate(var_name = fct_reorder(var_name, importance))
ggplot(x, aes(y = var_name, x = importance)) +
geom_vline(xintercept = 0) +
geom_point() +
theme_minimal()
mush_tree <- rpart(class ~ ., data = select(mush, -edible, -odor),
model = T, maxdepth=7)
rpart.plot(mush_tree)
loan <- read_csv('week_05_age_balance.csv')
ggplot(loan) +
geom_point(aes(x = balance, y = age, shape = loan, color = loan), size = 5) +
theme_minimal()
ggplot(loan) +
geom_point(aes(x = balance, y = age, shape = loan, color = loan), size = 5) +
geom_abline(slope = -.33, intercept = 61,
linetype = 2, size = 1) +
theme_minimal() +
scale_x_continuous(labels = scales::dollar) +
labs(title = "Can safe loans and risky loans be separated?") +
xlab("Bank balance ($K)") + ylab("Age")
loan_tree <- rpart(loan ~ balance + age, data = loan)
rpart.plot(loan_tree)
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(gridExtra)
library(grid)
library(caret)
library(randomForest)
library(e1071)
library(vtable)
# Data Cleaning and EDA.
diabetes_data = read_csv("diabetes.csv", show_col_types = FALSE) #Import the data
diabetes_data$Outcome = as.factor(diabetes_data$Outcome) #Ensure Outcome is a factor
#Managing missing values
diabetes_data = diabetes_data %>% #Replace all missing values with the means of those rows.
mutate(across(
where(is.numeric) & !c("Pregnancies", "Outcome"),
~ ifelse(. == 0, mean(.[. != 0], na.rm = TRUE), .) #if variable = 0 then replace with the mean of the column
))
#Create temp dataframe for summary table excluding outcome
temp = select(diabetes_data, -Outcome)
# Create a summary table using sumtable()
st(temp, col.breaks = 8,
summ =
c('notNA(x)', 'mean(x)',
'median(x)', 'sd(x)',
'min(x)', 'max(x)'),
summ.names =
c('N', 'Mean', 'Median', 'SD', 'Min', 'Max'),
, out= "latex"
)
#Creating grouped df for stacked bar plots on Age
grouped_df_age = diabetes_data %>%
mutate(Age_cat = case_when(
Age >= 0 & Age <= 9 ~ "0–9",
Age >= 10 & Age <= 19 ~ "10–19",
Age >= 20 & Age <= 29 ~ "20–29",
Age >= 30 & Age <= 39 ~ "30–39",
Age >= 40 & Age <= 49 ~ "40–49",
Age >= 50 & Age <= 59 ~ "50–59",
Age >= 60 & Age <= 69 ~ "60–69",
Age >= 70 & Age <= 79 ~ "70–79",
Age >= 80 & Age <= 89 ~ "80–89" #Group data into categories
)) %>% group_by(Age_cat, Outcome) %>%
summarise(Count = n(), .groups = "drop") %>%
group_by(Age_cat) %>%
mutate(Percentage = Count / sum(Count) * 100)
#Create Stacked Bar Plot
p1 = ggplot(grouped_df_age, aes(x = Age_cat, y = Percentage, fill = as.factor(Outcome))) +
geom_bar(stat = "identity", position = "stack") +
labs(
x = "Age",
y = "Percentage",
fill = "Diabetes Status",
) +
scale_fill_manual(values = c("lightgrey", "black"), labels = c("No Diabetes", "Diabetes")) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
)
#Creating grouped df for stacked bar plot on BMI
grouped_df_BMI = diabetes_data %>%
mutate(BMI_cat = case_when(
BMI >= 18 & BMI < 20 ~ "18-19",
BMI >= 20 & BMI < 30 ~ "20-29",
BMI >= 30 & BMI < 40 ~ "30-39",
BMI >= 40 & BMI < 50 ~ "40-49",
BMI >= 50 & BMI < 60 ~ "50-59",
BMI >= 60 & BMI <= 68 ~ "60-68" #Group data into categories
)) %>%
group_by(BMI_cat, Outcome) %>%
summarise(Count = n(), .groups = "drop") %>%
group_by(BMI_cat) %>%
mutate(Percentage = Count / sum(Count) * 100)
# Generate the stacked bar plot
p2 = ggplot(grouped_df_BMI, aes(x = BMI_cat, y = Percentage, fill = as.factor(Outcome))) +
geom_bar(stat = "identity", position = "stack") +
labs(
x = "BMI",
y = "Percentage",
fill = "Diabetes Status",
) +
scale_fill_manual(values = c("lightgrey", "black"), labels = c("No Diabetes", "Diabetes")) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
)
#Creating grouped df for stacked bar plot on Glucose
grouped_df_glu = diabetes_data %>%
mutate(glu_cat = case_when(
Glucose >= 50 & Glucose <= 75 ~ "50-75",
Glucose >= 76 & Glucose <= 100 ~ "76-100",
Glucose >= 101 & Glucose <= 125 ~ "101-125",
Glucose >= 126 & Glucose <= 150 ~ "126-150",
Glucose >= 151 & Glucose <= 175 ~ "151-175",
Glucose >= 176 & Glucose <= 200 ~ "176-200" #Group data into categories
)) %>%
mutate(glu_cat = factor(glu_cat, levels = c("50-75", "76-100", "101-125", "126-150", "151-175", "176-200"))) %>% #Make sure glu_cat is in factor in a specific order
group_by(glu_cat, Outcome) %>%
summarise(Count = n(), .groups = "drop") %>%
group_by(glu_cat) %>%
mutate(Percentage = Count / sum(Count) * 100)
# Generate the plot
p3 = ggplot(grouped_df_glu, aes(x = glu_cat, y = Percentage, fill = as.factor(Outcome))) +
geom_bar(stat = "identity", position = "stack") +
labs(
x = "Glucose (mg/dl)",
y = "Percentage",
fill = "Diabetes Status",
) +
scale_fill_manual(values = c("lightgrey", "black"), labels = c("No Diabetes", "Diabetes")) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
)
#Creating grouped df for stacked bar plot on Insulin
grouped_df_in = diabetes_data %>%
mutate(in_cat = case_when(
Insulin >= 0 & Insulin <= 200 ~ "0-200",
Insulin >= 201 & Insulin <= 400 ~ "201-400",
Insulin >= 401 & Insulin <= 600 ~ "401-600",
Insulin >= 601 & Insulin <= 800 ~ "601-800",
Insulin >= 801 & Insulin <= 1000 ~ "801-1000", #Group data into categories
)) %>%
group_by(in_cat, Outcome) %>%
summarise(Count = n(), .groups = "drop") %>%
group_by(in_cat) %>%
mutate(Percentage = Count / sum(Count) * 100)
# Generate the plot
p4 = ggplot(grouped_df_in, aes(x = in_cat, y = Percentage, fill = as.factor(Outcome))) +
geom_bar(stat = "identity", position = "stack") +
labs(
x = "Insulin (mu U/ml)",
y = "Percentage",
fill = "Diabetes Status",
) +
scale_fill_manual(values = c("lightgrey", "black"), labels = c("No Diabetes", "Diabetes")) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
)
#Combine plots with a single caption
grid.arrange(p1, p2, p3, p4, ncol = 2,
bottom = textGrob("Figure 1: Distribution of Diabetes Status Across Key Factors", gp = gpar(fontsize = 9), hjust = 0.15),
heights = c(1, 1)
)
#Scaling
continuous_features = names(diabetes_data)[sapply(diabetes_data, is.numeric) & names(diabetes_data) != "Outcome"] #create df with only continuous variables
diabetes_data[continuous_features] = scale(diabetes_data[continuous_features]) #Z-score normalisation
#Split the dataset into test and training data
set.seed(123)
split = sample(1:nrow(diabetes_data), 0.8 * nrow(diabetes_data))
train_data = diabetes_data[split,]
test_data = diabetes_data[-split,]
#Random Forest Model
rf_model = randomForest(Outcome ~ ., data = train_data, ntree = 1000, importance = TRUE) #Create the RF model
print(rf_model) #Used to see OOB error rates to determine best number of trees
rf_predictions = predict(rf_model, newdata = test_data) #Tests the RF model by making predictions on the test data.
conf_matrix = confusionMatrix(rf_predictions, test_data$Outcome) #Creates the confusion matrix
accuracyRF = conf_matrix$overall["Accuracy"] #Get the accuracy of the RF model
#Generate importance values
importance_values = importance(rf_model)
feature_importance = data.frame(
Feature = rownames(importance_values),
Importance = importance_values[, 2]
) %>%
arrange(desc(Importance))
#Plot the feature importance bar graph
ggplot(feature_importance, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "grey", colour = "black") +
coord_flip() +
labs(
x = "Factor",
y = "Importance",
caption = "Figure 2: Importance of Features Based on Mean Decrease in Accuracy") +
theme_minimal()
#AUC
library(pROC)   # For AUC calculation
rf_prob <- predict(rf_model, newdata = test_data, type = "prob")[,2]  # Probability for the positive class
roc_obj <- roc(test_data$Outcome, rf_prob)  # Create ROC curve object
auc_value_RF <- round(auc(roc_obj),3)
print(auc_value_RF)
# ROC Curve
# Plot ROC Curve
ggplot(data = data.frame(TPR = rev(roc_obj$sensitivities), FPR = rev(1 - roc_obj$specificities)),
aes(x = FPR, y = TPR)) +
geom_line(color = "blue", size = 1) +  # ROC curve
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +  # Diagonal reference line
labs(
title = "ROC Curve for Random Forest Model",
x = "False Positive Rate (1 - Specificity)",
y = "True Positive Rate (Sensitivity)",
caption = paste("AUC:", auc_value_RF)
) +
theme_minimal()
# Conf Matrix
# Convert confusion matrix to a data frame for visualization
conf_df <- as.data.frame(conf_matrix$table)
# Rename columns for better readability
colnames(conf_df) <- c("Reference", "Prediction", "Freq")
# Plot the confusion matrix
ggplot(conf_df, aes(x = Prediction, y = Reference, fill = Freq)) +
geom_tile(color = "black") +  # Tile plot for heatmap effect
geom_text(aes(label = Freq), size = 6) +  # Show frequency values
scale_fill_gradient(low = "white", high = "darkgrey") +  # Color scale
labs(
x = "Predicted Class",
y = "Actual Class",
fill = "Frequency",
caption = "Figure: Confusion Matrix for the Random Forest Model"
) +
theme_minimal()
#Performance Metrics
accuracyRF <- round(conf_matrix$overall["Accuracy"],3)
precisionRF <- round(conf_matrix$byClass["Pos Pred Value"],3)
recallRF <- round(conf_matrix$byClass["Sensitivity"],3)
f1_scoreRF <- round(2 * (precisionRF * recallRF) / (precisionRF + recallRF),3)
diabetes_data$Outcome = factor(diabetes_data$Outcome, levels = c(0, 1), labels = c(0, 1)) #Ensure Outcome is a factor
#Split the dataset into test and training data
set.seed(123)
split = sample(1:nrow(diabetes_data), 0.8 * nrow(diabetes_data))
train_data = diabetes_data[split, ]
test_data = diabetes_data[-split, ]
LR_model = glm(Outcome ~ ., data = train_data, family = "binomial") #Code to create the LR model
#Code to evaluate the model
LR_probabilities = predict(LR_model, newdata = test_data, type = "response") # Predict probabilities on the test dataset
LR_predictions = ifelse(LR_probabilities > 0.5, 1, 0) # Convert probabilities to factor predictions. if probability is over 0.5 outcome is 1 if not it is 0
conf_matrix = table(Predicted = LR_predictions, Actual = test_data$Outcome) #Create conf matrix
# Calculate odds ratios
odds_ratios <- exp(coef(LR_model))
# Create dataframe for plotting
odds_ratios_df <- data.frame(
Predictor = names(odds_ratios),
Odds_Ratio = odds_ratios,
Lower_CI = exp(confint(LR_model)[, 1]),  # Lower 95% CI
Upper_CI = exp(confint(LR_model)[, 2])   # Upper 95% CI
)
# Remove intercept for better visualization
odds_ratios_df <- odds_ratios_df %>% filter(Predictor != "(Intercept)")
# Create the forest plot
ggplot(odds_ratios_df, aes(x = reorder(Predictor, Odds_Ratio), y = Odds_Ratio)) +
geom_point(size = 4, color = "blue") +  # Point estimates for odds ratios
geom_errorbar(aes(ymin = Lower_CI, ymax = Upper_CI), width = 0.2, color = "black") +  # Confidence intervals
geom_hline(yintercept = 1, linetype = "dashed", color = "red") +  # Reference line at OR = 1
coord_flip() +  # Flip coordinates for a horizontal forest plot
labs(
x = "Predictors",
y = "Odds Ratio (95% CI)",
title = "Forest Plot of Odds Ratios from Logistic Regression",
caption = "Confidence intervals computed at 95% level"
) +
theme_minimal()
# Conf Matrix
# Convert confusion matrix to a data frame for visualization
conf_df <- as.data.frame(conf_matrix)
# Rename columns for better readability
colnames(conf_df) <- c("Reference", "Prediction", "Freq")
# Plot the confusion matrix
ggplot(conf_df, aes(x = Prediction, y = Reference, fill = Freq)) +
geom_tile(color = "black") +  # Tile plot for heatmap effect
geom_text(aes(label = Freq), size = 6) +  # Show frequency values
scale_fill_gradient(low = "white", high = "darkgrey") +  # Color scale
labs(
x = "Predicted Class",
y = "Actual Class",
fill = "Frequency",
caption = "Figure: Confusion Matrix for the Logistic Regression Model"
) +
theme_minimal()
# Compute ROC curve and AUC score
roc_obj <- roc(test_data$Outcome, LR_probabilities, levels = c(0, 1), direction = "<")
auc_value_LR <- round(auc(roc_obj),3)
# Print AUC value
print(paste("AUC:", auc_value_LR))
# Plot ROC Curve
ggplot(data = data.frame(TPR = rev(roc_obj$sensitivities), FPR = rev(1 - roc_obj$specificities)),
aes(x = FPR, y = TPR)) +
geom_line(color = "blue", size = 1) +  # ROC curve
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +  # Reference diagonal
labs(
title = "ROC Curve for Logistic Regression Model",
x = "False Positive Rate (1 - Specificity)",
y = "True Positive Rate (Sensitivity)",
caption = paste("AUC:", auc_value_LR)
) +
theme_minimal()
#Performance Metrics
accuracyLR = round(sum(diag(conf_matrix)) / sum(conf_matrix),3) #Calculate the accuracy of the model
# Define confusion matrix values
TP <- 29  # True Positives (Diabetic correctly predicted)
FP <- 23  # False Positives (Non-Diabetic incorrectly predicted as Diabetic)
FN <- 12  # False Negatives (Diabetic incorrectly predicted as Non-Diabetic)
TN <- 90  # True Negatives (Non-Diabetic correctly predicted)
# Calculate Precision
precisionLR <- round(TP / (TP + FP),3)
# Calculate Recall (Sensitivity)
recallLR <- round(TP / (TP + FN),3)
f1_scoreLR <- round(2 * (precisionLR * recallLR) / (precisionLR + recallLR),3)
#SVM Model
svm_model <- svm(Outcome ~ ., data = train_data, gamma = 0.1, probability = TRUE)
svm_predictions = predict(svm_model, newdata = test_data)
svm_probabilities <- predict(svm_model, newdata = test_data, probability = TRUE)
conf_matrix = confusionMatrix(svm_predictions, test_data$Outcome) #Creates the confusion matrix
prob_matrix <- attr(svm_probabilities, "probabilities")
svm_prob <- prob_matrix[, 1]
# Compute ROC curve and AUC score
roc_obj <- roc(test_data$Outcome, svm_prob, levels = c(0, 1), direction = "<")
auc_value_SVM <- round(auc(roc_obj),2)
# Print AUC value
print(paste("AUC:", auc_value_SVM))
# Plot ROC Curve
ggplot(data = data.frame(TPR = rev(roc_obj$sensitivities), FPR = rev(1 - roc_obj$specificities)),
aes(x = FPR, y = TPR)) +
geom_line(color = "blue", size = 1) +  # ROC curve
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +  # Reference diagonal
labs(
title = "ROC Curve for SVM Model",
x = "False Positive Rate (1 - Specificity)",
y = "True Positive Rate (Sensitivity)",
caption = paste("AUC:", auc_value_SVM)
) +
theme_minimal()
#Code to create the confusion matrix
conf_df = as.table(conf_matrix$table)%>%
as.data.frame() #converts confusion matrix into a df
colnames(conf_df) = c("Reference", "Prediction", "Freq") #alter conf_df column names
#Plots confusion matrix
ggplot(conf_df, aes(x = Prediction, y = Reference, fill = Freq)) +
geom_tile(color = "black") +
geom_text(aes(label = Freq), size = 6) +
scale_fill_gradient(low = "white", high = "darkgrey") +
labs(
x = "Predicted Class",
y = "Actual Class",
fill = "Frequency",
caption = "Figure 5: Confusion Matrix for the SVM model"
) +
theme_minimal()
# Perfomance Metrics
accuracySVM = round(conf_matrix$overall["Accuracy"],3)
precisionSVM = round(conf_matrix$byClass["Pos Pred Value"],3)
recallSVM = round(conf_matrix$byClass["Sensitivity"],3)
f1_scoreSVM = round(2 * (precisionSVM * recallSVM) / (precisionSVM + recallSVM),3)
#Create accuracy  df
accuracy_df = data.frame(
Model = c("Random Forest", "Logistic Regression", "SVM"),
Value = c(accuracyRF, accuracyLR, accuracySVM)
)
#Code to create bar graph showing accuracy for each model
ggplot(accuracy_df, aes(x = Model, y = Value, fill = Model)) +
geom_bar(stat = "identity", color = "black") +
geom_text(aes(label = round(Value, 2)), vjust = -0.5, size = 5) +
labs(
x = "Model",
y = "Value",
caption = "Figure 6: Accuracy for each model") +
theme_minimal() +
ylim(0, 1.3)  +
scale_fill_grey(start = 0.9, end = 0.1)
#Table of all performance metrics
# Create a dataframe for the performance metrics
performance_df <- data.frame(
Model = c("Random Forest", "SVM", "Logistic Regression"),
Accuracy = c(accuracyRF, accuracySVM, accuracyLR),
Precision = c(precisionRF, precisionSVM, precisionLR),
Recall = c(recallRF, recallSVM, recallLR),
F1_Score = c(f1_scoreRF, f1_scoreSVM, f1_scoreLR),
AUC = c(auc_value_RF, auc_value_SVM, auc_value_LR)
)
# Display table using kable for better formatting
tablePerf = kable(performance_df, caption = "Performance Metrics for Each Model")
tablePerf
#SVM Model
svm_model <- svm(Outcome ~ ., data = train_data, gamma = 0.1, probability = TRUE)
svm_predictions = predict(svm_model, newdata = test_data)
svm_probabilities <- predict(svm_model, newdata = test_data, probability = TRUE)
conf_matrix = confusionMatrix(svm_predictions, test_data$Outcome) #Creates the confusion matrix
prob_matrix <- attr(svm_probabilities, "probabilities")
svm_prob <- prob_matrix[, 1]
# Compute ROC curve and AUC score
roc_obj <- roc(test_data$Outcome, svm_prob, levels = c(0, 1), direction = "<")
auc_value_SVM <- round(auc(roc_obj),3)
# Print AUC value
print(paste("AUC:", auc_value_SVM))
# Plot ROC Curve
ggplot(data = data.frame(TPR = rev(roc_obj$sensitivities), FPR = rev(1 - roc_obj$specificities)),
aes(x = FPR, y = TPR)) +
geom_line(color = "blue", size = 1) +  # ROC curve
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +  # Reference diagonal
labs(
title = "ROC Curve for SVM Model",
x = "False Positive Rate (1 - Specificity)",
y = "True Positive Rate (Sensitivity)",
caption = paste("AUC:", auc_value_SVM)
) +
theme_minimal()
#Code to create the confusion matrix
conf_df = as.table(conf_matrix$table)%>%
as.data.frame() #converts confusion matrix into a df
colnames(conf_df) = c("Reference", "Prediction", "Freq") #alter conf_df column names
#Plots confusion matrix
ggplot(conf_df, aes(x = Prediction, y = Reference, fill = Freq)) +
geom_tile(color = "black") +
geom_text(aes(label = Freq), size = 6) +
scale_fill_gradient(low = "white", high = "darkgrey") +
labs(
x = "Predicted Class",
y = "Actual Class",
fill = "Frequency",
caption = "Figure 5: Confusion Matrix for the SVM model"
) +
theme_minimal()
# Perfomance Metrics
accuracySVM = round(conf_matrix$overall["Accuracy"],3)
precisionSVM = round(conf_matrix$byClass["Pos Pred Value"],3)
recallSVM = round(conf_matrix$byClass["Sensitivity"],3)
f1_scoreSVM = round(2 * (precisionSVM * recallSVM) / (precisionSVM + recallSVM),3)
